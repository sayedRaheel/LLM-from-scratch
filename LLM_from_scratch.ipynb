{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM from scratch Series"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stage 1: Data Preperation and sampling, Attention Mechanism, LLM architecture\n",
    "\n",
    "BUILDING AN LLM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stage 2: Training Loop, Model Evaluation, Loading Pretrained Weights\n",
    "\n",
    "FOUNDATION Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stage 3: Finetuning \n",
    "\n",
    "Classifier OR Personal Assistant "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Satge 1: Data Prep & Sampling-Tokenization\n",
    "How do you prepare inpur text for training LLMs?\n",
    "\n",
    "Step 1: Splitting text into individual word and subword token\n",
    "\n",
    "Step 2: Converting tokens in token IDs\n",
    "\n",
    "step 3: Encode token IDs into vector representations\n",
    "\n",
    "\n",
    "## GPT Tokenization Process\n",
    "\n",
    "<img src=\"tokenization.png\" alt=\"GPT Tokenization Flow\" width=\"800\"/>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Cretaing tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding =\"utf-8\") as f:\n",
    "    raw_text= f.read()\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'text']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text= \"Hello, world. This, is a text\"\n",
    "result= re.split(r'(\\s)',text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'text']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text= \"Hello, world. This, is a text\"\n",
    "result= re.split(r'([,.]|\\s)',text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'text']\n"
     ]
    }
   ],
   "source": [
    "result= [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', '--', 'a', 'text', '?']\n"
     ]
    }
   ],
   "source": [
    "#simple tokenization scheme\n",
    "import re\n",
    "text= \"Hello, world. This, is-- a text?\"\n",
    "result=re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "\n",
    "result= [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result= re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "result= [item.strip() for item in result if item.strip()]\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'HAD',\n",
       " 'always',\n",
       " 'thought',\n",
       " 'Jack',\n",
       " 'Gisburn',\n",
       " 'rather',\n",
       " 'a',\n",
       " 'cheap',\n",
       " 'genius',\n",
       " '--',\n",
       " 'though',\n",
       " 'a',\n",
       " 'good',\n",
       " 'fellow',\n",
       " 'enough',\n",
       " '--',\n",
       " 'so',\n",
       " 'it',\n",
       " 'was',\n",
       " 'no',\n",
       " 'great',\n",
       " 'surprise',\n",
       " 'to',\n",
       " 'me',\n",
       " 'to',\n",
       " 'hear',\n",
       " 'that',\n",
       " ',',\n",
       " 'in']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[:30]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Creating Token ids\n",
    "\n",
    "Creating Vocabulary in alphabetical form of tokens and assigning unique to each unique token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words= sorted(set(result))\n",
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab={token:integers for integers, token in enumerate(all_words)}\n",
    "#later when we get an ouput from LLM it eill be in numbers so we need inverse version of vocabulary\n",
    "# that converts toke IDs to back to corresponding text tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make a tokinizer class in Python that have encode(text token to token ids) method and decode method(tokens is to text token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int= vocab\n",
    "        self.int_to_str= {id:token for token, id in vocab.items()}\n",
    "\n",
    "    def encode(self,text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        #removing the unnecessary space\n",
    "        preprocessed= [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids= [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "\n",
    "    def decode(self,id):\n",
    "        text=\" \".join([self.int_to_str[i] for i in id])\n",
    "        #replace spaces before the specified punctuations\n",
    "        text= re.sub(r'\\s+([,.:;?_!\"()\\'])',r'\\1',text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens= SimpleTokenizer(vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "id=tokens.encode(\"This is \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.decode(id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADDING SPECIAL CONTEXT TOKENS\n",
    "We will modify the python class to handle unknown tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADDING SPECIAL CONTEXT TOKENS\n",
    "\n",
    "In the previous section, we implemented a simple tokenizer and applied it to a passage\n",
    "from the training set. \n",
    "\n",
    "In this section, we will modify this tokenizer to handle unknown\n",
    "words.\n",
    "\n",
    "\n",
    "In particular, we will modify the vocabulary and tokenizer we implemented in the\n",
    "previous section, SimpleTokenizerV2, to support two new tokens, <|unk|> and\n",
    "<|endoftext|>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "We can modify the tokenizer to use an <|unk|> token if it\n",
    "encounters a word that is not part of the vocabulary. \n",
    "\n",
    "Furthermore, we add a token between\n",
    "unrelated texts. \n",
    "\n",
    "For example, when training GPT-like LLMs on multiple independent\n",
    "documents or books, it is common to insert a token before each document or book that\n",
    "follows a previous text source\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the end of text and unkown tokens in the Vocabulary\n",
    "all_tokens= sorted(list(set(result)))\n",
    "all_tokens.extend([\"<|endoftext|>\",\"<|unk|>\"])\n",
    "vocab={tokens: integer for integer,tokens in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "for i,item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Tokenizer class to handle new tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int= vocab\n",
    "        self.int_to_str= {id:token for token, id in vocab.items()}\n",
    "\n",
    "    def encode(self,text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        #removing the unnecessary space\n",
    "        preprocessed= [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed= [item if item in self.str_to_int else '<|unk|>' for item in preprocessed]\n",
    "        ids= [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "\n",
    "    def decode(self,id):\n",
    "        text=\" \".join([self.int_to_str[i] for i in id])\n",
    "        #replace spaces before the specified punctuations\n",
    "        text= re.sub(r'\\s+([,.:;?_!\"()\\'])',r'\\1',text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer= SimpleTokenizerV2(vocab)\n",
    "text1=\"Hello, do you like tea?\" \n",
    "text2=\"In the sunlit terraces of the places.\"\n",
    "text= \" <|endoftext|> \".join((text1,text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, do you like tea? <|endoftext|> In the sunlit terraces of the places.'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text=text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization algorithm\n",
    "- Word based\n",
    "- Sub-word based\n",
    "- Chaarcter based"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte Pair encoding- Sub word encoding\n",
    "\n",
    "- Rule 1: Do not split frequently used words into smaller subwords\n",
    "- Rule 2: Split the rare words into smaller, meaningful subwords\n",
    "\n",
    "Example: \"boy\" should not be split, \"boys\" should be split into \"boy\" and \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /Users/sayedraheel/miniconda3/lib/python3.11/site-packages/google_images_download-2.8.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tiktoken in /Users/sayedraheel/miniconda3/lib/python3.11/site-packages (0.7.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/sayedraheel/miniconda3/lib/python3.11/site-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/sayedraheel/miniconda3/lib/python3.11/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sayedraheel/miniconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sayedraheel/miniconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sayedraheel/miniconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sayedraheel/miniconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "#print('tiktoken version:', importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer= tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18435, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271]\n"
     ]
    }
   ],
   "source": [
    "text= (\" Hello, do you like tea? <|endoftext|> In the sunlit terraces\" \"of someunknownPlace\")\n",
    "integers= tokenizer.encode(text,allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Input-Target pairs\n",
    "- The last step before we create vector embeddings is to create input-target pairs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    raw_text= f.read()\n",
    "\n",
    "enc_text= tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_window=4\n",
    "x= enc_sample[:context_window]\n",
    "y=enc_sample[1:context_window+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ---> 4920\n",
      "[290, 4920] ---> 2241\n",
      "[290, 4920, 2241] ---> 287\n",
      "[290, 4920, 2241, 287] ---> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,context_window+1):\n",
    "    context= enc_sample[:i]\n",
    "    desired= enc_sample[i]\n",
    "    print(context,\"--->\",desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and --->  established\n",
      " and established --->  himself\n",
      " and established himself --->  in\n",
      " and established himself in --->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,context_window+1):\n",
    "    context= enc_sample[:i]\n",
    "    desired= enc_sample[i]\n",
    "    print(tokenizer.decode(context),\"--->\",tokenizer.decode([desired]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPLEMENTING A DATA LOADER\n",
    "- Our goal is to create a data loader that creates two tensors a input tensor that the LLM sees and a target tensor that includes the tagets for the LLMs to predict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building A Dataset Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building A Dataset Class\n",
    "\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length,stride):\n",
    "\n",
    "        self.input_ids=[]\n",
    "        self.target_ids=[]\n",
    "\n",
    "        #tokenize the entire text\n",
    "        tokens_id= tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        for i in range(0, len(tokens_id)- max_length,stride):\n",
    "            input_chunk=tokens_id[i:i+max_length]\n",
    "            target_chunk=tokens_id[i+1: i+1+max_length+1]\n",
    "\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaderv1(text,batch_size=4, max_length=256,\n",
    "                        stride=128,shuffle=True,drop_last=True,\n",
    "                        num_worker=0):\n",
    "    tokenizer= tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    #Create the dataset\n",
    "    dataset= GPTDatasetV1(text,tokenizer,max_length,stride)\n",
    "\n",
    "    #Create the dataloader\n",
    "    dataloader= DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_worker\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=create_dataloaderv1(raw_text,batch_size=2, max_length=6,\n",
    "                        stride=128,shuffle=True,drop_last=True,\n",
    "                        num_worker=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[22645,    11,   465, 10904,  4252,  6236],\n",
      "        [  503,  4291,   262,  4252, 18250,  8812]]) tensor([[   11,   465, 10904,  4252,  6236,   429, 25839],\n",
      "        [ 4291,   262,  4252, 18250,  8812,   558,    13]])\n",
      "tensor([[  26,  475,  314, 2936,  683, 1969],\n",
      "        [ 286,  616, 4286,  705, 1014,  510]]) tensor([[ 475,  314, 2936,  683, 1969, 2157,  502],\n",
      "        [ 616, 4286,  705, 1014,  510,   26,  475]])\n",
      "tensor([[18560,   438,  7091,   750,   523,   765],\n",
      "        [ 1021,   757,   438, 10919,   257,   410]]) tensor([[  438,  7091,   750,   523,   765,   683,   705],\n",
      "        [  757,   438, 10919,   257,   410,  5040,   329]])\n",
      "tensor([[   11, 17728,   257,  8500,  4417,   284],\n",
      "        [ 2612,  4369,    11,   523,   326,   612]]) tensor([[17728,   257,  8500,  4417,   284,   670,   319],\n",
      "        [ 4369,    11,   523,   326,   612,   550,   587]])\n",
      "tensor([[  673,   373, 10032,   286,   852, 13055],\n",
      "        [   13,   198,   198,     1, 19242,   339]]) tensor([[  373, 10032,   286,   852, 13055,   366, 34751],\n",
      "        [  198,   198,     1, 19242,   339,   442, 17758]])\n",
      "tensor([[4150,    8, 3688,  284,  402,  271],\n",
      "        [  40,  367, 2885, 1464, 1807, 3619]]) tensor([[    8,  3688,   284,   402,   271, 10899,   338],\n",
      "        [  367,  2885,  1464,  1807,  3619,   402,   271]])\n",
      "tensor([[  673,  1908,   329,   345,  1701,   198],\n",
      "        [  271, 10899,   550,   366,  7109, 14655]]) tensor([[ 1908,   329,   345,  1701,   198,   198,     1],\n",
      "        [10899,   550,   366,  7109, 14655,   683,   866]])\n",
      "tensor([[  423,  4750,   326,  9074,    13,   402],\n",
      "        [10197,   832,   262, 46475,   286, 18113]]) tensor([[ 4750,   326,  9074,    13,   402,   271, 10899],\n",
      "        [  832,   262, 46475,   286, 18113,   544,   338]])\n",
      "tensor([[41186, 39614,  1386,    11,   287,   262],\n",
      "        [   11,   508,   550, 18459,  1068,   284]]) tensor([[39614,  1386,    11,   287,   262, 13203,  5482],\n",
      "        [  508,   550, 18459,  1068,   284,  1577,   257]])\n",
      "tensor([[ 1310,  1165,   881, 40642,   972,  6654],\n",
      "        [  257,  2726,  6227,   284,  1833,   683]]) tensor([[ 1165,   881, 40642,   972,  6654,   832,   616],\n",
      "        [ 2726,  6227,   284,  1833,   683,  1365,    13]])\n",
      "tensor([[ 1459,   714,  1239,   423,  4499,   326],\n",
      "        [19672,   484,   550,   262,  1295,   286]]) tensor([[  714,  1239,   423,  4499,   326, 18680,   510],\n",
      "        [  484,   550,   262,  1295,   286, 15393,   438]])\n",
      "tensor([[  520,  5493,  6776,   878,   502,    11],\n",
      "        [11061,   340,    11,  3114,   510,   379]]) tensor([[5493, 6776,  878,  502,   11,  290,  284],\n",
      "        [ 340,   11, 3114,  510,  379,  262, 4286]])\n",
      "tensor([[  329,   502,     0,   383,   520,  5493],\n",
      "        [  616,   705, 23873,  2350,     6, 14707]]) tensor([[  502,     0,   383,   520,  5493,    82,  1302],\n",
      "        [  705, 23873,  2350,     6, 14707,   588,   257]])\n",
      "tensor([[  286,  1762,    30,  2011, 29483,  2540],\n",
      "        [  198,  1544, 13818,  4622,    11,  1231]]) tensor([[ 1762,    30,  2011, 29483,  2540,   284,   467],\n",
      "        [ 1544, 13818,  4622,    11,  1231, 35987,    11]])\n",
      "tensor([[  284,  1234,  8737,   656, 19133,   553],\n",
      "        [  314,   550,  1775,   683,    11,   523]]) tensor([[ 1234,  8737,   656, 19133,   553,   373,   530],\n",
      "        [  550,  1775,   683,    11,   523,  1690,    11]])\n",
      "tensor([[  262,  1633,   286, 24380,   329, 20728],\n",
      "        [ 2951,   319,   262,  1327, 22674,  1022]]) tensor([[ 1633,   286, 24380,   329, 20728,   287,   257],\n",
      "        [  319,   262,  1327, 22674,  1022,    13,  5845]])\n",
      "tensor([[12036,   683,     0,  3226,  1781,   314],\n",
      "        [ 3347, 27846,   503,  2048,  4628, 24882]]) tensor([[  683,     0,  3226,  1781,   314,  4001,   284],\n",
      "        [27846,   503,  2048,  4628, 24882,   379,   262]])\n",
      "tensor([[ 273, 1807,  673,  750,   13,  887],\n",
      "        [ 314, 4752,  340, 6777,   13,  632]]) tensor([[1807,  673,  750,   13,  887,  673, 3521],\n",
      "        [4752,  340, 6777,   13,  632,  373,  407]])\n",
      "tensor([[ 383, 8631, 3872,  373,   11,  314],\n",
      "        [1092,  517,  621,  611,  314, 1549]]) tensor([[ 8631,  3872,   373,    11,   314,  1422,   470],\n",
      "        [  517,   621,   611,   314,  1549,  1239, 12615]])\n",
      "tensor([[  438,   292,   339,   550,   587,   832],\n",
      "        [ 6164,    25,   366, 16773,   290,   766]]) tensor([[  292,   339,   550,   587,   832,    11,   290],\n",
      "        [   25,   366, 16773,   290,   766,   262,  1334]])\n"
     ]
    }
   ],
   "source": [
    "for i, n in data:\n",
    "    print(i,n)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Embeddings/Vector Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "### Token Embeddings\n",
    "- Dense vector representations that map discrete tokens into continuous vector space\n",
    "- Each token (word/subword) becomes a fixed-length vector of floating-point numbers\n",
    "- Dimension typically ranges from 256 to 1024 based on model size\n",
    "\n",
    "### Why Embeddings Matter\n",
    "- Transform symbolic text into numerical format for neural networks\n",
    "- Capture semantic relationships between tokens\n",
    "- Enable mathematical operations on language\n",
    "- Foundation for contextual representations\n",
    "\n",
    "## Architecture Components\n",
    "\n",
    "### 1. Vocabulary System\n",
    "- Token dictionary mapping words to unique IDs\n",
    "- Handles unknown tokens (UNK)\n",
    "- Special tokens (PAD, BOS, EOS)\n",
    "- Vocabulary size impacts model complexity\n",
    "\n",
    "### 2. Embedding Layer\n",
    "- Learned weight matrix mapping token IDs to vectors\n",
    "- Initialized randomly, refined during training\n",
    "- Shared across model's encoder/decoder components\n",
    "- Enables parameter sharing and efficient learning\n",
    "\n",
    "### 3. Positional Encoding\n",
    "- Adds position information to token embeddings\n",
    "- Can be learned or fixed sinusoidal encodings\n",
    "- Essential for capturing token order in sequences\n",
    "\n",
    "## Training Process\n",
    "\n",
    "### 1. Initialization\n",
    "- Random initialization of embedding weights\n",
    "- Xavier/Glorot or other suitable initialization methods\n",
    "- Embedding dimension chosen based on vocabulary size\n",
    "\n",
    "### 2. Learning\n",
    "- Updated via backpropagation\n",
    "- Learns from contextual predictions\n",
    "- Optimized to capture semantic relationships\n",
    "\n",
    "### 3. Output\n",
    "- Final embeddings reflect learned language patterns\n",
    "- Similar words cluster in vector space\n",
    "- Enables meaningful vector arithmetic\n",
    "\n",
    "## Key Considerations\n",
    "\n",
    "### Dimensionality\n",
    "- Higher dimensions capture more nuanced relationships\n",
    "- Trade-off between expressiveness and computation\n",
    "- Must balance with model size constraints\n",
    "\n",
    "### Context Window\n",
    "- Size of context affects learning quality\n",
    "- Longer contexts capture broader relationships\n",
    "- Impacts computational requirements\n",
    "\n",
    "### Normalization\n",
    "- L2 normalization often applied\n",
    "- Helps stabilize training\n",
    "- Prevents embedding magnitude issues\n",
    "\n",
    "This overview covers the foundational concepts for implementing embeddings in language models from scratch. Each component builds upon the previous, creating a complete system for neural language understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Token embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import trained word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "model= api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.64892578e-02 -1.64062500e-01 -7.01904297e-03  2.79296875e-01\n",
      "  8.88671875e-02  1.89453125e-01 -2.29492188e-02  1.32812500e-01\n",
      "  4.08203125e-01 -2.70996094e-02 -1.28906250e-01 -9.17968750e-02\n",
      "  4.10156250e-02 -1.75781250e-02  1.21582031e-01  1.49414062e-01\n",
      "  3.97949219e-02  4.41894531e-02 -3.63769531e-02 -1.58203125e-01\n",
      "  2.72216797e-02 -1.97753906e-02 -3.51562500e-02 -2.62451172e-02\n",
      " -3.73535156e-02  3.61328125e-02 -8.05664062e-02  2.51953125e-01\n",
      "  2.69531250e-01  5.43594360e-05 -3.19824219e-02  1.43432617e-02\n",
      " -2.31445312e-01  1.52343750e-01 -2.83203125e-01 -4.39453125e-01\n",
      " -3.06640625e-01  2.70996094e-02 -1.63085938e-01 -1.03515625e-01\n",
      " -2.17773438e-01  3.20312500e-01  5.05371094e-02  1.81640625e-01\n",
      "  1.62109375e-01 -1.55639648e-02 -3.97949219e-02  1.91406250e-01\n",
      "  9.47265625e-02 -2.08984375e-01 -2.06054688e-01 -3.27148438e-02\n",
      "  1.21093750e-01 -6.68334961e-03 -2.96020508e-03  9.08203125e-02\n",
      "  1.44195557e-03  2.09960938e-02  2.89306641e-02  1.75781250e-01\n",
      "  1.63085938e-01  1.30859375e-01 -2.30468750e-01  3.32031250e-01\n",
      " -4.17480469e-02 -1.57470703e-02 -3.14453125e-01  1.79687500e-01\n",
      " -9.52148438e-02  1.49414062e-01  1.36718750e-01 -1.13281250e-01\n",
      "  3.12500000e-01 -1.19140625e-01 -3.16406250e-01  6.59179688e-02\n",
      " -1.68945312e-01 -6.73828125e-02  1.35498047e-02 -3.22265625e-01\n",
      "  2.04101562e-01  2.21679688e-01  7.81250000e-02  2.38281250e-01\n",
      " -1.19140625e-01 -1.59179688e-01  1.69921875e-01  3.18359375e-01\n",
      "  2.31933594e-03  1.26953125e-01 -2.98828125e-01 -1.69921875e-01\n",
      " -1.42578125e-01 -2.89062500e-01 -1.65039062e-01 -2.96875000e-01\n",
      "  1.04003906e-01  3.57421875e-01  1.75781250e-01 -5.68847656e-02\n",
      " -1.68945312e-01 -1.90429688e-01  7.08007812e-02  1.53320312e-01\n",
      " -4.49218750e-02  2.10937500e-01  6.59179688e-02 -1.48437500e-01\n",
      "  4.17968750e-01 -2.16796875e-01 -1.49414062e-01 -2.25585938e-01\n",
      "  9.13085938e-02 -6.29882812e-02 -3.10058594e-02 -4.54101562e-02\n",
      " -4.05273438e-02 -2.18750000e-01  2.41210938e-01  1.62109375e-01\n",
      " -7.56835938e-02 -1.57226562e-01 -1.89453125e-01  9.71679688e-02\n",
      "  1.60156250e-01  1.16699219e-01 -1.76757812e-01 -5.88378906e-02\n",
      " -1.26953125e-01  1.16699219e-01 -1.37695312e-01 -3.12500000e-01\n",
      " -8.34960938e-02  2.65625000e-01 -5.46875000e-02  1.96289062e-01\n",
      "  9.08203125e-02  2.02148438e-01 -5.31250000e-01 -7.51953125e-02\n",
      "  6.07299805e-03 -1.33789062e-01 -3.36914062e-02 -7.95898438e-02\n",
      " -1.14257812e-01 -7.12890625e-02  3.90625000e-03  3.61328125e-02\n",
      " -1.69921875e-01 -2.21679688e-01  3.96484375e-01 -7.08007812e-02\n",
      " -5.81054688e-02  2.30468750e-01 -2.66113281e-02  1.15722656e-01\n",
      " -2.73437500e-01 -1.94335938e-01 -2.57812500e-01  3.55468750e-01\n",
      " -8.74023438e-02 -1.35742188e-01  1.35742188e-01 -4.71191406e-02\n",
      " -9.08203125e-02  2.81250000e-01  3.24707031e-02 -8.39843750e-02\n",
      " -2.61718750e-01 -2.35351562e-01 -2.18505859e-02  8.64257812e-02\n",
      " -1.52343750e-01  1.46484375e-02 -4.25781250e-01 -4.27246094e-02\n",
      "  3.00781250e-01 -1.84570312e-01 -1.45507812e-01  1.79687500e-01\n",
      " -1.16210938e-01 -2.53906250e-01 -1.68945312e-01 -2.85156250e-01\n",
      "  1.42578125e-01  2.61230469e-02 -8.98437500e-02  1.75781250e-02\n",
      "  3.73535156e-02  1.86767578e-02 -9.17968750e-02 -5.93261719e-02\n",
      " -2.75390625e-01  8.05664062e-03 -4.78515625e-02  5.54687500e-01\n",
      "  4.90722656e-02 -5.39550781e-02  4.05273438e-02  1.84326172e-02\n",
      " -3.83300781e-02  6.17675781e-02  6.73828125e-02  2.35351562e-01\n",
      "  2.47070312e-01  2.78320312e-02 -1.71875000e-01 -5.73730469e-02\n",
      " -1.91406250e-01 -8.98437500e-02  1.72851562e-01  2.36328125e-01\n",
      " -4.12109375e-01 -2.63671875e-01 -1.13769531e-01  1.66015625e-01\n",
      " -1.01074219e-01  2.44140625e-01 -5.27343750e-01 -7.71484375e-02\n",
      " -1.56250000e-01 -1.86523438e-01  8.44726562e-02  3.90625000e-01\n",
      " -2.47070312e-01  3.88183594e-02 -2.51953125e-01  1.11328125e-01\n",
      " -4.19921875e-02 -3.04687500e-01 -7.91015625e-02 -5.17578125e-02\n",
      " -1.64794922e-02 -4.92187500e-01 -1.25976562e-01 -1.71875000e-01\n",
      " -8.78906250e-02 -2.19726562e-02  6.64062500e-02  2.79296875e-01\n",
      "  7.12890625e-02 -9.86328125e-02 -2.25830078e-02 -2.25585938e-01\n",
      "  6.44531250e-02 -3.07617188e-02 -1.98242188e-01 -2.50000000e-01\n",
      "  1.91406250e-01  2.75878906e-02 -2.25585938e-01  1.90429688e-01\n",
      " -6.00585938e-02  9.58251953e-03  5.39550781e-02  8.10546875e-02\n",
      "  1.39648438e-01  1.55273438e-01  1.30004883e-02  1.41143799e-03\n",
      "  1.76757812e-01  2.01171875e-01 -3.49121094e-02 -5.66406250e-02\n",
      " -9.76562500e-02  2.01171875e-01 -1.94335938e-01 -1.62109375e-01\n",
      " -3.10546875e-01  7.27539062e-02  1.86767578e-02 -1.42578125e-01\n",
      "  4.51660156e-02  2.24609375e-01  9.08203125e-02 -1.68945312e-01\n",
      "  2.05078125e-01 -2.83203125e-01 -8.93554688e-02  6.15234375e-02\n",
      "  2.48046875e-01  1.50146484e-02 -9.32617188e-02  2.37304688e-01\n",
      " -3.12500000e-02 -1.04980469e-01  3.07617188e-02  5.63964844e-02\n",
      "  7.76367188e-02  2.10937500e-01 -2.73437500e-01 -9.72747803e-04\n",
      "  9.46044922e-03 -1.60156250e-01  4.32128906e-02 -1.62109375e-01\n",
      " -3.95507812e-02  8.59375000e-02 -3.18359375e-01  3.12500000e-02]\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "word_vector=model\n",
    "print(word_vector['laptop'])\n",
    "print(word_vector['laptop'].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### King + Woman - Man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.4827326238155365), ('queens', 0.466781347990036), ('kumaris', 0.4653734564781189), ('kings', 0.4558638632297516), ('womens', 0.422832190990448), ('princes', 0.4176960587501526), ('Al_Anqari', 0.41725507378578186), ('concubines', 0.40110787749290466), ('monarch', 0.39624831080436707), ('monarchy', 0.39430150389671326)]\n"
     ]
    }
   ],
   "source": [
    "#example of using most_similar\n",
    "print(word_vector.most_similar(positive=['king','women'], negative=['man'],topn=10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us check the similarity b/w a few pair of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2883053\n",
      "0.6510956\n",
      "0.03228098\n",
      "0.5061663\n",
      "0.84866095\n"
     ]
    }
   ],
   "source": [
    "# Example of calculating similarity\n",
    "print(word_vector.similarity('women','man'))\n",
    "print(word_vector.similarity('king','queen'))\n",
    "print(word_vector.similarity('icecream','diabetes'))\n",
    "print(word_vector.similarity('sugar','honey'))\n",
    "print(word_vector.similarity('uncle','nephew'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('towers', 0.8531749844551086), ('skyscraper', 0.6417425870895386), ('Tower', 0.639177143573761), ('spire', 0.5946877598762512), ('responded_Understood_Atlasjet', 0.5931612849235535)]\n"
     ]
    }
   ],
   "source": [
    "print(word_vector.most_similar('tower',topn=5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How are token embeddings are created for large language models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Token Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs_ids= torch.tensor([2,3,5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab_size= 6\n",
    "output_dim=3\n",
    "torch.manual_seed(123)\n",
    "\n",
    "embedding_layer= torch.nn.Embedding(vocab_size,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
